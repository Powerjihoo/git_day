# Manual [GAON LLM]

## ëª©ì°¨

* [1.ê°œìš”](#1.-ê°œìš”)
* [2. ì‹œìŠ¤í…œ ê¸°ëŠ¥ ì •ì˜](#2-ì‹œìŠ¤í…œ-ê¸°ëŠ¥-ì •ì˜)

  + [2.1 Architecture](#21-Architecture)
  + [2.2 Chunking](#22-Chunking)
  + [2.3 LLM](#23-LLM)
  + [2.4 Demo](#24-Demo)
* [3. í†µí•© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜](#3-í†µí•©-ë¼ì´ë¸ŒëŸ¬ë¦¬-ì„¤ì¹˜)
* [4.Chunking](#4-Chunking)
  * [4.1 hwp2md](#41-hwp2md)
  * [4.2 base_chunking](#42-base_chunking)
  * [4.3 Font_based_chunking](#43-Font_based_chunking)
* [5.LLM](#5-LLM)
* [6.Demo](#6-Demo)

<div style="page-break-after: always;"></div>

## 1. ê°œìš”

   GAON-LLMì€ ê³„ì•½ì—°êµ¬ì„¼í„°(ê°€ì˜¨í”Œë«í¼, ê³ ë ¤ëŒ€)ì—ì„œ ê°œë°œí•œ í•œêµ­ì–´ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í”„ë ˆì„ì›Œí¬ë¡œ, SaaS LLM(Software-as-a-Service based on Large Language Models) ë˜ëŠ” LLM ë‚©í’ˆì„ ìœ„í•œ ì›ì²œê¸°ìˆ  ì—°êµ¬ë¥¼ ëª©í‘œë¡œ í•¨. ì‚¬ìš©í™˜ê²½ ë° ì„¤ì¹˜ ë°©ë²•ì€ [ë‹¤ìŒ](#3-ì‚¬ìš©-í™˜ê²½)ê³¼ ê°™ìŠµë‹ˆë‹¤.



## 2.  Architecture

![System Architecture](./assets/System%20Architecture.png)



## 3. í†µí•© í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

   GAON-LLMì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê° ê¸°ëŠ¥ì„ ì›í™œí•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ë¬¸ì„œ ì²­í‚¹, LLMì˜ í•™ìŠµ ë° ì‹¤í–‰, ê·¸ë¦¬ê³  DEMO ì‹œìŠ¤í…œ êµ¬í˜„ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ ìì„¸í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡ì€ [Link]()ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- OS : ubuntu 22.04
- Lang : `python3.8`
- CUDA_Versioin: 12.3

#### ì‹œìŠ¤í…œ ì‚¬ì–‘ 
- í•™ìŠµ: A6000 GPU(48G) 8ì¥  
- ì¶”ë¡ (ë°ëª¨): A6000 GPU(48G) 1ì¥


##### ê°€ìƒí™˜ê²½ ì„¤ì •

```shell
$ conda create -n gaon python=3.8

$ conda activate gaon
```

##### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜


~~~shell
(gaon)$ pip install -r requirements.txt
~~~


## 4. Chunking

### 4.1 hwp2md

   ë°ì´í„°ë¥¼ htmlíŒŒì¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ëª¨ë“ˆ. í•´ë‹¹ ëª¨ë“ˆì„ ê¸°ë°˜ìœ¼ë¡œ base_chunkingì„ ìˆ˜í–‰

#### Docker ì´ë¯¸ì§€ ë¹Œë“œ

- ë„ì»¤ ì´ë¯¸ì§€ ìƒì„±

  í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ `./Chunking/hwp2md` ë””ë ‰í† ë¦¬ ì•ˆì˜ Docker ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

  ```shell
  docker build -t hwp2md ./Chunking/hwp2md
  ```

#### ì‚¬ìš©

- hwp2html

    HWP íŒŒì¼ì„ ì½ì–´ì„œ HTML íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ```shell
    docker run -it --rm \
        -v $(pwd)/[ì…ë ¥ íŒŒì¼ ê²½ë¡œ]:/usr/src/app/input.hwp \
        -v $(pwd)/[ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ]:/usr/src/app/output/ \
        hwp2md hwp2html input.hwp [ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ]/[ì¶œë ¥ íŒŒì¼ ì´ë¦„]
    ```

    - ì˜ˆì‹œ) Chunking/hwp2md/src/MOA_MOB.hwpë¥¼ output/output.htmlë¡œ ë³€í™˜
    ```shell
    docker run -it --rm \
        -v $(pwd)/Chunking/hwp2md/src/MOA_MOB.hwp:/usr/src/app/input.hwp \
        -v $(pwd)/output:/usr/src/app/output/ \
        hwp2md hwp2html input.hwp output/output.html
    ```

- html2md

    HTML íŒŒì¼ì„ ì½ì–´ì„œ MD íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ```shell
    docker run -it --rm \
        -v $(pwd)/[ì…ë ¥ íŒŒì¼ ê²½ë¡œ]:/usr/src/app/input.html \
        -v $(pwd)/[ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ]:/usr/src/app/output/ \
        hwp2md html2md input.html [ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ]/[ì¶œë ¥ íŒŒì¼ ì´ë¦„].md
    ```

    - ì˜ˆì‹œ) output/output.htmlì„ output/output.mdë¡œ ë³€í™˜
    ```shell
    docker run -it --rm \
        -v $(pwd)/output/output.html:/usr/src/app/input.html \
        -v $(pwd)/output:/usr/src/app/output/ \
        hwp2md html2md input.html output/output.md
    ```


### 4.2 base_chunking

   html íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ë¶„í• í•œ ë’¤, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Semantic ê¸°ë°˜ Chunkingì„ ìˆ˜í–‰

```h

```





### 4.3 Font_based_chunking

 Font sizeë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„ì„í•´ Chunkingì„ ìˆ˜í–‰

```
```





<div style="page-break-after: always;"></div>



## 5. LLM

ë³¸ ì„¹ì…˜ì—ì„œëŠ” Instruction Tuningê³¼ Inference ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.



### 5.1 Instruction-Tuning

Instruction Tuningì€ LLMì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. êµ¬ì¶•ëœ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì´ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¡°ì •ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì•„ë˜ì˜ ëª…ë ¹ì–´ëŠ” `meta-llama/Llama-3.1-8B-Instruct` ëª¨ë¸ì„ í™œìš©í•œ Instruction Tuningì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:

```python


LR=2e-5 # 5e-06

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nproc_per_node=8 finetune.py \
    --base_model "meta-llama/Llama-3.1-8B-Instruct" \
    --output_dir "./ckpt/llama3" \
    --vram_available "48GB" \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --num_epochs 5 \
    --learning_rate $LR \
    --cutoff_len 4096 \
    --warmup_steps 100 \
    --logging_steps 1 \
    --add_eos_token True\
    --bf16 \
    --save_steps 50\
    --eval_steps 50\
    --save_total_limit 4\
    --data_path "./dataset/training.jsonl"   
```

Learning Rate, batch_size, epoch ë“± ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



### 5.2 Inference

Inference ë‹¨ê³„ëŠ” í›ˆë ¨ëœ ëª¨ë¸ì„ ì‹¤ì œë¡œ ì ìš©í•˜ì—¬ ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë¸ì´ í•™ìŠµí•œ ë‚´ìš©ì„ í† ëŒ€ë¡œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë‹¨ê³„ë¡œ, ì‚¬ìš©ìëŠ” ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ í†µí•´ ì‰½ê²Œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from transformers import AutoTokenizer, pipeline
import torch

model_and_tokenizer_path = "{model_path}"
tokenizer = AutoTokenizer.from_pretrained(model_and_tokenizer_path)

pipeline = pipeline(
    "text-generation",
    model=model_and_tokenizer_path,
    tokenizer=tokenizer,
    model_kwargs={"torch_dtype": "auto"},
    device="cuda:0",
)


#chat_template
# messages = [
#     {"role": "user", 
#      "content": "ì´ìˆœì‹  ì¥êµ°ì´ ëˆ„êµ¬ì•¼?"},
# ]
# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) # Don't use `pipeline.tokenizer`


#W/O chat_template
prompt='ì´ìˆœì‹  ì¥êµ°ì´ ëˆ„êµ¬ì•¼?'

outputs = pipeline(
    prompt,
    max_new_tokens=1028,
    add_special_tokens=True
)

print(outputs[0]["generated_text"][len(prompt):]) # 'ì´ìˆœì‹  ì¥êµ°ì€ ì¡°ì„  ì‹œëŒ€ì˜ ì¥êµ°ìœ¼ë¡œ ì„ì§„ì™œë€ ë‹¹ì‹œ ì¡°ì„ ì˜ ìˆ˜êµ°ì„ ì´ëŒê³  ì¼ë³¸ì˜ ê³µê²©ìœ¼ë¡œë¶€í„° ë³´í˜¸í•œ ì¸ë¬¼ì…ë‹ˆë‹¤.ë§ˆì¹¨ë‚´ ì´ìˆœì‹  ì¥êµ°ì€ 1592ë…„ì— ì¡°ì„  ìˆ˜êµ°ì„ ì´ëŒê³  ì¼ë³¸ì˜ ê³µê²©ìœ¼ë¡œë¶€í„° ë³´í˜¸í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.'
```





## 6. DEMO

ì´ ë¬¸ì„œëŠ” GAON-LLM í”„ë¡œì íŠ¸ì˜ DEMO ì‹œìŠ¤í…œì— ëŒ€í•œ êµ¬ì„± ìš”ì†Œ ë° ì‹¤í–‰ ì§€ì¹¨ì„ ì„¤ëª…í•©ë‹ˆë‹¤. DEMO ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(UI) ì œê³µì„ í†µí•´ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ê³¼ ë‹¤ì–‘í•œ ê²€ìƒ‰ ê¸°ëŠ¥ì˜ ì‹¤ì œ ì ìš©ì„ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. 

![DEMO](./assets/demo.png)


1. `/app`

   >  \_\_init\_\_.py: Flask ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì´ˆê¸°í™”í•˜ê³  Blueprint ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” íŒŒì¼
   >
   > generate.py: ì–¸ì–´ ëª¨ë¸, ì„ë² ë”© ëª¨ë¸, ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì½”ë“œê°€ í¬í•¨
   >
   > routes.py: ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë¼ìš°íŒ… ê¸°ëŠ¥ì„ ê´€ë¦¬
   >
   > templates/index.html: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë·°í˜ì´ì§€ ì„¤ì •ì„ ì •ì˜
   >
   > utils.py: ê²€ìƒ‰ ë° ë¬¸ì„œ ê²€ìƒ‰ì— í•„ìš”í•œ ì½”ë“œë¥¼ í¬í•¨

2. `/bm25`

   > {Files}: ì €ì¥ëœ ë¬¸ì„œì™€ Sparse Tensor
   >
   > bm25.py: ì²­í‚¹ëœ ë¬¸ì„œë¥¼ BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ Sparse Vectorë¡œ ë³€í™˜í•˜ê³ , ì´ì™€ í•¨ê»˜ ë¬¸ì„œë¥¼ ì €ì¥

3. `/db`

   > {Files}: ì €ì¥ëœ ë¬¸ì„œ ë° Dense Tensor 
   >
   > processing.py: Embedding ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Dense Vectorë¥¼ ë§Œë“¤ê³  ë¬¸ì„œì™€ í•¨ê»˜ ì €ì¥

4. `/model`: LLM ëª¨ë¸
5. `main.py`: ì›¹ ë°ëª¨ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ì½”ë“œ íŒŒì¼ë¡œ, ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë™í•˜ê¸° ìœ„í•œ ì„¤ì •



#### ğŸ”¥ì›¹ ë°ëª¨ êµ¬ë™ ì‹œ í™•ì¸ì‚¬í•­ğŸ”¥

1. `main.py` ì—ì„œ **port number** í™•ì¸ (ì™¸ë¶€ í¬íŠ¸ ì‚¬ìš© ì‹œ)
2. `app/generate.py`
   1. `initialize_models()`ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ë° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì´ë¦„ì„ í™•ì¸. ì´ˆê¸°í™” ì‹œ ì§€ì •ëœ ì´ë¦„ê³¼ ì¼ì¹˜
   2. `load_db()`,`load_bm` ê²½ë¡œë¥¼ ì •í™•íˆ ì„¤ì •
3. `app/routes.py`
   1. `index()`: LLM model name check [ì´ˆê¸°í™” ì‹œ ëª¨ë¸ ì´ë¦„ê³¼ ë™ì¼]
   2. `predict_model()`: embed_model name check [ì´ˆê¸°í™” ì‹œ ëª¨ë¸ ì´ë¦„ê³¼ ë™ì¼]


```sh
(gaon)$CUDA_VISIBLE_DEVICES={gpu_number} python3 main.py
```

<div style="page-break-after: always;"></div>

## 6. ë¶€ë¡ 
