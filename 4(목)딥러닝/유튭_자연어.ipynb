{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 표준 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the policy.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 형태소 level에서 분석을 하면 다른 품사 or 다른 시제의 단어라 해도 같은 형태로 토큰화 할수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'ate', 'eaten', 'eat']\n",
      "['eat', 'at', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "stem1 = PorterStemmer()\n",
    "stem2 = LancasterStemmer()\n",
    "\n",
    "words= ['eat','ate','eaten','eating']\n",
    "print([stem1.stem(v) for v in words])\n",
    "print([stem2.stem(v) for v in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 4가지 단어를 다르게 분류하면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'eat', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "lemm = WordNetLemmatizer()\n",
    "words= ['eat','ate','eaten','eating']\n",
    "print([lemm.lemmatize(w, pos = 'v') for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전부 다 eat(동사)로 추출 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'shoud', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
      "['We', 'shoud', 'study', 'hard', 'exam', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\p\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_sentence = \"We shoud all study hard for the exam.\"\n",
    "stop_words= set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(input_sentence)\n",
    "result = []\n",
    "\n",
    "for w in word_tokens:\n",
    "  if w not in stop_words:\n",
    "    result.append(w)\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정수 인토딩 및 Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course : English, Number : 0\n",
      "Course : Math, Number : 1\n",
      "Course : Science, Number : 2\n"
     ]
    }
   ],
   "source": [
    "mylist = ['English','Math','Science']\n",
    "for n, name in enumerate(mylist):\n",
    "  print(\"Course : {}, Number : {}\".format(name,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-frequency sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
      "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab = {'apple':2, 'July':6, 'piano':4, 'cup':8, 'orange':1}\n",
    "vocab_sort = sorted(vocab.items(), key = lambda x:x[1] , reverse =True)\n",
    "print(vocab_sort)\n",
    "\n",
    "word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)}\n",
    "print(word2inx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'polish': 10, ',': 11, 'but': 12, 'some': 13, 'of': 14, 'algorithms': 15, 'have': 16}\n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the polish, \"\\\n",
    "      \"but some of Model-based RL algorithms do have a value function\"\n",
    "\n",
    "token_text = tokenizer.tokenize(text)\n",
    "word2inx = {}\n",
    "Bow = []\n",
    "\n",
    "for word in token_text:\n",
    "  if word not in word2inx.keys():\n",
    "    word2inx[word] = len(word2inx)\n",
    "    Bow.insert(len(word2inx)-1,1)\n",
    "  else:\n",
    "    inx = word2inx.get(word)\n",
    "    Bow[inx] +=1\n",
    "\n",
    "print(word2inx)\n",
    "print(Bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7071067811865475 0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cos_sim(A,B):\n",
    "  return np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "\n",
    "a = [1,0,0,1]\n",
    "b = [0,1,1,0]\n",
    "c = [1,1,1,1]\n",
    "\n",
    "print(cos_sim(a,b), cos_sim(a,c), cos_sim(b,c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
